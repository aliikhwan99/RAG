{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "building basic - RAG",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliikhwan99/RAG/blob/main/building_basic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG (Retrieval-Augmented Generation)** is a framework that **combines** retrieval-based methods with **generative models**. It enhances the capabilities of a **generative model** (like OpenAI's GPT or T5) by using **external knowledge** sources during the generation process, improving factual accuracy and relevance.\n",
        "\n",
        "# What is RAG?\n",
        "\n",
        "* Retrieval-Augmented: The system retrieves relevant information from a knowledge base or external documents (like Wikipedia, custom datasets, etc.) to provide context.\n",
        "\n",
        "* Generation: A generative model uses the retrieved information to produce coherent and contextually enriched responses or outputs.\n",
        "\n",
        "* RAG frameworks are widely used in applications like question answering, summarization, and document generation where factual accuracy is critical."
      ],
      "metadata": {
        "id": "nWsLHIko7t2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Build RAG in Python\n",
        "\n",
        "You can build a RAG pipeline using libraries like Hugging Face Transformers, faiss (for dense vector search), and OpenAI embeddings. Here's a high-level approach\n",
        "\n",
        "# Step 1: Define Your Knowledge Base\n",
        "\n",
        "* Collect your data: This could be documents, articles, or any textual information you want the model to retrieve from.\n",
        "* Preprocess the data: Tokenize, clean, and structure it for embedding generation.\n",
        "\n",
        "# Step 2: Embed the Knowledge Base\n",
        "\n",
        "Use embeddings to convert your textual data into vector representations.\n",
        "\n",
        "* Libraries: Use sentence-transformers (e.g., sentence-transformers/all-mpnet-base-v2) or OpenAI's embedding models."
      ],
      "metadata": {
        "id": "O_WPN7e57t2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:12:47.744715Z",
          "iopub.execute_input": "2024-12-09T14:12:47.745102Z",
          "iopub.status.idle": "2024-12-09T14:13:00.588874Z",
          "shell.execute_reply.started": "2024-12-09T14:12:47.745071Z",
          "shell.execute_reply": "2024-12-09T14:13:00.587478Z"
        },
        "id": "_Q_gVd1I7t2b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sentence_transformers\n",
        "print(\"sentence-transformers version:\", sentence_transformers.__version__)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:13:00.591556Z",
          "iopub.execute_input": "2024-12-09T14:13:00.592039Z",
          "iopub.status.idle": "2024-12-09T14:13:25.674538Z",
          "shell.execute_reply.started": "2024-12-09T14:13:00.591988Z",
          "shell.execute_reply": "2024-12-09T14:13:25.673315Z"
        },
        "id": "G2JVVW_A7t2c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Sample data\n",
        "documents = [\"This is a document about AI.\", \"Another document on machine learning.\"]\n",
        "embeddings = model.encode(documents, convert_to_tensor=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:13:25.675779Z",
          "iopub.execute_input": "2024-12-09T14:13:25.676492Z",
          "iopub.status.idle": "2024-12-09T14:13:31.290611Z",
          "shell.execute_reply.started": "2024-12-09T14:13:25.676416Z",
          "shell.execute_reply": "2024-12-09T14:13:31.289387Z"
        },
        "id": "Bv649fIl7t2c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Set Up a Retrieval System\n",
        "Store the embeddings for fast retrieval.\n",
        "\n",
        "Use faiss for approximate nearest neighbor (ANN) search."
      ],
      "metadata": {
        "id": "PGnXjdER7t2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:13:31.292985Z",
          "iopub.execute_input": "2024-12-09T14:13:31.293363Z",
          "iopub.status.idle": "2024-12-09T14:13:43.79407Z",
          "shell.execute_reply.started": "2024-12-09T14:13:31.293317Z",
          "shell.execute_reply": "2024-12-09T14:13:43.792585Z"
        },
        "id": "jDiXxvsB7t2d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Convert embeddings to numpy\n",
        "embeddings_np = np.array([emb.numpy() for emb in embeddings])\n",
        "\n",
        "# Indexing\n",
        "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance\n",
        "index.add(embeddings_np)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:23:25.49634Z",
          "iopub.execute_input": "2024-12-09T14:23:25.496828Z",
          "iopub.status.idle": "2024-12-09T14:23:25.504085Z",
          "shell.execute_reply.started": "2024-12-09T14:23:25.496792Z",
          "shell.execute_reply": "2024-12-09T14:23:25.502522Z"
        },
        "id": "OaW7v7VF7t2e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Build the Generative Model\n",
        "Fine-tune or use a pre-trained generative model (like GPT or T5).\n",
        "\n",
        "Integrate with Hugging Face's transformers"
      ],
      "metadata": {
        "id": "15_qUka27t2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:13:44.308865Z",
          "iopub.execute_input": "2024-12-09T14:13:44.309401Z",
          "iopub.status.idle": "2024-12-09T14:13:47.579322Z",
          "shell.execute_reply.started": "2024-12-09T14:13:44.309364Z",
          "shell.execute_reply": "2024-12-09T14:13:47.577928Z"
        },
        "id": "fnnRDXuj7t2e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Combine Retrieval and Generation\n",
        "**1.Accept a query, retrieve top-k relevant documents using faiss:**"
      ],
      "metadata": {
        "id": "bM-p1i6s7t2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load SentenceTransformer model for embeddings\n",
        "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Generate query embedding\n",
        "query = \"What is AI?\"\n",
        "query_embedding = embedding_model.encode([query])[0]  # This produces a NumPy array\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:13:47.581028Z",
          "iopub.execute_input": "2024-12-09T14:13:47.58153Z",
          "iopub.status.idle": "2024-12-09T14:13:48.743795Z",
          "shell.execute_reply.started": "2024-12-09T14:13:47.581476Z",
          "shell.execute_reply": "2024-12-09T14:13:48.742426Z"
        },
        "id": "nL5hF7GY7t2f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Append retrieved documents to the query and generate a response:**"
      ],
      "metadata": {
        "id": "iXxcH7KR7t2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = f\"Question: {query} Context: {retrieved_docs[0]}\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "outputs = model.generate(**inputs)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:40:51.10951Z",
          "iopub.execute_input": "2024-12-09T14:40:51.110044Z",
          "iopub.status.idle": "2024-12-09T14:41:00.621488Z",
          "shell.execute_reply.started": "2024-12-09T14:40:51.110003Z",
          "shell.execute_reply": "2024-12-09T14:41:00.620393Z"
        },
        "id": "psU0EE_f7t2f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = []\n",
        "\n",
        "\n",
        "if retrieved_docs:\n",
        "    input_text = f\"Question: {query} Context: {retrieved_docs[0]}\"\n",
        "else:\n",
        "    print(\"No documents retrieved. Please check the retrieval process.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:39:06.82432Z",
          "iopub.execute_input": "2024-12-09T14:39:06.825137Z",
          "iopub.status.idle": "2024-12-09T14:39:06.83278Z",
          "shell.execute_reply.started": "2024-12-09T14:39:06.825077Z",
          "shell.execute_reply": "2024-12-09T14:39:06.831421Z"
        },
        "id": "xgm-hMZc7t2f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Example documents\n",
        "documents = [\"This is a document about AI.\", \"Another document on machine learning.\"]\n",
        "\n",
        "# Generate embeddings (mock embeddings for simplicity)\n",
        "embeddings = np.random.rand(len(documents), 768).astype(\"float32\")  # Mock embeddings\n",
        "index = faiss.IndexFlatL2(768)  # Initialize FAISS index\n",
        "index.add(embeddings)\n",
        "\n",
        "# Query\n",
        "query = \"What is AI?\"\n",
        "query_embedding = np.random.rand(768).astype(\"float32\")  # Mock query embedding\n",
        "D, I = index.search(np.array([query_embedding]), k=1)\n",
        "\n",
        "# Retrieve documents\n",
        "retrieved_docs = [documents[i] for i in I[0]]\n",
        "\n",
        "# Generate response\n",
        "if retrieved_docs:\n",
        "    input_text = f\"Question: {query} Context: {retrieved_docs[0]}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(**inputs)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"No documents retrieved.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T14:39:15.543752Z",
          "iopub.execute_input": "2024-12-09T14:39:15.546982Z",
          "iopub.status.idle": "2024-12-09T14:39:39.478336Z",
          "shell.execute_reply.started": "2024-12-09T14:39:15.544389Z",
          "shell.execute_reply": "2024-12-09T14:39:39.47644Z"
        },
        "id": "ti0GEdVV7t2f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "JT_zdtYw7t2f"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}